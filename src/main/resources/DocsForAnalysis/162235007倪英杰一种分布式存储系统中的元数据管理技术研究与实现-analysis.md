索取号： [TP315/10.411]{.ul} 密级： [公开]{.ul}

> ![](media/image1.png){width="3.4583333333333335in" height="0.71875in"}
>
> **工 程 硕 士 学 位 论 文**
>
> ![](media/image2.png){width="1.125in" height="0.9847222222222223in"}

**一种分布式存储系统中的**

**元数据管理技术研究与实现**

  **研 究 生：**       **[倪英杰]{.ul}**
  -------------------- ---------------------------------
  **指导教师：**       **[吉根林 教授]{.ul}**
  **培养单位：**       **[计算机科学与技术学院]{.ul}**
  **专业学位领域：**   **[计算机技术]{.ul}**
  **完成时间：**       **[2018年3月27日]{.ul}**
  **答辩时间：**       **[2018年5月11日]{.ul}**

**Research and Implementation of Metadata Management Technology in a
Distributed Storage System**

A Thesis Submitted to

Nanjing Normal University

For the Degree of Master of Engineering

BY

**Yingjie Ni**

Supervised by

**Prof. Genlin Ji**

School of Computer Science and Technology

Nanjing Normal University

March 2018

**学位论文独创性声明**

本人郑重声明：所提交的学位论文是本人在导师指导下进行的研究工作和取得的研究成果。本论文中除引文外，所有实验、数据和有关材料均是真实的。本论文中除引文和致谢的内容外，不包含其他人或其它机构已经发表或撰写过的研究成果。其他同志对本研究所做的贡献均已在论文中作了声明并表示了谢意。

学位论文作者签名： 日 期：

**学位论文使用授权声明**

研究生在校攻读学位期间论文工作的知识产权单位属南京师范大学。学校有权保存本学位论文的电子和纸质文档，可以借阅或上网公布本学位论文的部分或全部内容，可以采用影印、复印等手段保存、汇编本学位论文。学校可以向国家有关机关或机构送交论文的电子和纸质文档，允许论文被查阅和借阅。（保密论文在解密后遵守此规定）

保密论文注释：本学位论文属于保密论文，**密级**： **保密期限**为 年。

学位论文作者签名： 指导教师签名：

日 期： 日 期：

# 摘要

随着网络和信息技术的飞速发展，全球数据日益增长。面对当前PB级的海量数据存储需求，传统的存储系统在容量和性能的扩展上存在瓶颈。分布式存储系统因为具有海量数据存储、高扩展性、高性能、高可靠性、高可用性的特点，目前得到广泛研究和应用。据统计，海量存储系统中有超过一半以上的系统操作是元数据操作。元数据操作成为制约分布式存储系统的性能和扩展性的一个严重的瓶颈，因此对于分布式存储系统中元数据管理的研究不仅紧跟目前发展的趋势，而且具有重要的应用价值。本文在研究和分析了现有的元数据管理模型的基础上，研究并实现了一种分布式存储系统中的元数据管理技术，取得的成果如下：

1.对现有的元数据管理技术进行概述，将元数据管理模型分成集中式元数据管理模型、分布式元数据管理模型和无元数据管理模型三种，分别对其阐述原理并给出相应的元数据管理模型图，列举若干对应的存储系统，分析各个模型存在的性能缺陷，同时也分析了分布式存储系统中的两种容错机制，分析了各自的优缺点。

2.设计了一种面向运营商领域、具有高可靠性和高可扩展性的分布式存储系统ZettastorDBS，将其总体架构分成两大模块：数据通路模块和控制通路模块，给出数据通路模块和访问关系图，以及控制通路模块和基本控制流构成图，并根据元数据管理需求研究了元数据管理基本流程所包含的内容。

3.实现了ZettastorDBS系统中的元数据管理功能。首先，给出元数据的组织形式和元数据库中表结构的设计；其次，详细介绍了元数据的各个状态设计以及各个状态之间的迁移实现过程；最后，对于元数据管理程序中的核心程序给出设计与实现过程，包括两个定时器和一个元数据重构接口。同时，在Linux环境下部署整个系统并对其进行功能性测试。测试结果表明，元数据管理部分可以充分满足用户对存储服务在元数据管理各方面的需求。

**关键词：**元数据管理，分布式存储系统，元数据管理模型，海量数据存储

[]{#_Toc510092041 .anchor}Abstract

With the rapid development of networks and information technologies,
global data is growing. In the face of the current PB-level massive data
storage requirements, traditional storage systems have bottlenecks in
capacity and performance expansion. Distributed storage systems have
been widely studied and applied because of the characteristics of
massive data storage, high scalability, high performance, high
reliability, and high availability. According to statistics, more than
half of system operations in mass storage systems are metadata
operations. Metadata operations have become a serious bottleneck that
limits the performance and scalability of distributed storage systems.
Therefore, the research on metadata management in distributed storage
systems not only follows the current development trend, but also has
important application value. Based on the research and analysis of the
existing metadata management model, this thesis designs and implements a
metadata management technology in a distributed storage system. The
achievements of this thesis are as follows:

1\. An overview of the existing metadata management techniques is
provided. The metadata management model is divided into three types: a
centralized metadata management model, a distributed metadata management
model, and a non-metadata management model. Each of them elaborates the
principle, gives the corresponding metadata management model diagram,
enumerates several corresponding storage systems, and analyzes the
performance defects of each.At the same time, two kinds of
fault-tolerance mechanisms in distributed storage systems are analyzed,
and their advantages and disadvantages are analyzed.

2\. A distributed storage system named ZettastorDBS is designed. The
system oriented to the field of operators has high reliability and high
scalability. The overall architecture is divided into two major modules:
the data path module and the control path module, giving the data path
module, access diagram, the control path module and the basic control
flow composition diagram.Based on the metadata management requirements,
the contents of the basic metadata management process are studied.

3\. Metadata management functionality in the ZettastorDBS system is
implemented. First, the organization of metadata and the design of the
table structure in the metadata database are given. Secondly, the design
of each state of metadata and the process of migration between states
are introduced in detail. Finally, the core program in the metadata
management program is introduced. The design and implementation process
are given, including two timers and a metadata reconstruction interface.
At the same time, the entire system is deployed and functionally tested
in the Linux environment. The test results show that the metadata
management part can fully meet the user\'s requirements for storage
services in all aspects of metadata management.

**Keywords**: metadata management, distributed storage system, metadata
management model, mass data storage

**目录**

[摘要 I](#摘要)

[Abstract II](#_Toc510092041)

[第1章 绪论 1](#第1章-绪论)

> [1.1 研究背景与研究意义 1](#研究背景与研究意义)
>
> [1.2 国内外研究现状 2](#_Toc515129420)
>
> [1.3 本文研究内容 3](#本文研究内容)
>
> [1.4 本文组织结构 4](#本文组织结构)

[第2章 元数据管理技术概述 5](#第2章-元数据管理技术概述)

> [2.1分布式存储系统 5](#分布式存储系统)
>
> [2.1.1分布式存储系统简介 5](#分布式存储系统简介)
>
> [2.1.2分布式存储系统应用 6](#分布式存储系统应用)
>
> [2.2元数据管理模型 8](#元数据管理模型)
>
> [2.2.1集中式元数据管理模型 8](#集中式元数据管理模型)
>
> [2.2.2分布式元数据管理模型 9](#分布式元数据管理模型)
>
> [2.2.3无元数据管理模型 10](#无元数据管理模型)
>
> [2.3分布式存储系统中的容错机制 11](#分布式存储系统中的容错机制)
>
> [2.3.1 Master/Slave容错机制 11](#masterslave容错机制)
>
> [2.3.2 Multi-Master容错机制 12](#multi-master容错机制)
>
> [2.4本章小结 13](#本章小结)

[第3章 分布式系统元数据管理方案设计
14](#第3章-分布式系统元数据管理方案设计)

> [3.1需求分析 14](#需求分析)
>
> [3.1.1分布式存储系统总体需求 14](#分布式存储系统总体需求)
>
> [3.1.2元数据管理需求 15](#元数据管理需求)
>
> [3.2分布式存储系统结构 16](#分布式存储系统结构)
>
> [3.2.1总体架构 16](#总体架构)
>
> [3.2.2数据通路模块 17](#数据通路模块)
>
> [3.2.3控制通路模块 19](#控制通路模块)
>
> [3.3元数据管理基本流程 20](#元数据管理基本流程)
>
> [3.4本章小结 21](#本章小结-1)

[第4章 分布式系统元数据管理的功能实现
22](#第4章-分布式系统元数据管理的功能实现)

> [4.1元数据库设计与实现 22](#元数据库设计与实现)
>
> [4.1.1相关术语解释 22](#相关术语解释)
>
> [4.1.2元数据组织形式 23](#元数据组织形式)
>
> [4.1.3元数据库表结构设计 26](#元数据库表结构设计)
>
> [4.2元数据状态迁移设计与实现 30](#元数据状态迁移设计与实现)
>
> [4.2.1元数据状态设计 30](#元数据状态设计)
>
> [4.2.2元数据状态迁移实现 32](#元数据状态迁移实现)
>
> [4.3元数据管理程序设计与实现 38](#元数据管理程序设计与实现)
>
> [4.3.1TimeoutSweeper定时器 38](#timeoutsweeper定时器)
>
> [4.3.2VolumeSweeper定时器 40](#volumesweeper定时器)
>
> [4.3.3元数据重构接口 43](#元数据重构接口)
>
> [4.4本章小结 45](#本章小结-2)

[第5章 元数据管理系统测试 46](#第5章-元数据管理系统测试)

> [5.1测试环境与方法 46](#测试环境与方法)
>
> [5.2系统测试结果 46](#系统测试结果)
>
> [5.3本章小结 50](#本章小结-3)

[第6章 总结与展望 51](#第6章-总结与展望)

> [6.1工作总结 51](#工作总结)
>
> [6.2工作展望 51](#工作展望)

[参考文献 53](#参考文献)

[致谢 56](#致谢)

# 第1章 绪论

## 1.1 研究背景与研究意义

随着网络和信息技术的飞速发展，各种互联网应用、电子商务平台、穿戴设备乃至各领域的科学研究检测设备都产生了海量的数据信息，我们已经进入大数据^\[1\]^时代。大数据时代下的数据存储需求更加多样，传统的数据存储系统已经难以满足大数据存储的需求。面对海量数据存储和处理的需求，分布式存储技术得到了广泛的应用^\[2\]^。并且，随着时代的发展和技术的日益更新，分布式存储技术被不断提出新的需求和要求：数据分布的地域空间更加广阔、数据存储量巨大、数据可靠性要求越来越高。分布式存储系统是将数据存储在物理上分散的多个存储节点上，对这些节点的资源进行统一的管理与分配，并向用户提供系统访问接口。

现实生活中，分布式存储技术在各行各业都有着广泛的应用。在医疗^\[3\]\[4\]^、农业^\[5\]\[6\]\[7\]^、教育^\[8\]^、地震勘测^\[9\]\[10\]^、天文^\[11\]\[12\]^、气象^\[13\]^、音频图像^\[14\]\[15\]\[16\]^等行业都有着重要的应用价值。尤其在医疗、天文、地震勘测等诸多行业内，行业数据需要保留数年，这就在带来海量数据的同时也带来大量的元数据，这些需求对分布式存储系统的可靠性和扩展性要求非常高。

对于大多数分布式存储系统而言，通常将元数据与数据独立开来，将元数据全部存储到元数据服务器中进行管理，即控制流与数据流进行分离^\[17\]\[18\]^。用户使用分布式存储系统时，先得到要操作的实际数据的元数据，定位到具体的位置，再进行相关操作。元数据管理是分布式存储系统的核心技术之一。元数据是一种用于描述数据的组织结构、数据在分布式环境中的存储位置等关键信息。没有元数据，数据的存储及访问就无法进行，元数据的管理及访问效率直接关系到数据的安全性、可靠性，以及数据访问处理性能。

本文研究一种分布式存储系统中的元数据管理技术，用来实现元数据的存储和重构。主要包括两部分，一是数据服务器中数据存储格式，采用数据块数据信息和数据块元数据信息共同存储的方式；二是元数据服务器中元数据重构过程，这部分由数据服务器将各自节点上的元数据信息定时汇报给元数据服务器，再由元数据服务器进行重构。本课题来源于南京鹏云网络科技有限公司，其研究任务是设计并实现一种分布式存储系统中的元数据管理技术。

## 1.2 国内外研究现状

分布式存储系统发展到现在已有40多年的历史，上世纪80年代出现了第一代分布式存储系统,，以NFS(Network
File System)和AFS(Andrew File
System)为代表。客户端用系统提供的接口来访问远程文件，客户更多的看重系统的数据可靠性和访问性能。随着Internet的普及和发展，基于光纤通道的SAN(Storage
Area Network)、NAS(Network Access
Server)网络存储技术得到广泛的应用，出现多种分布式体系结构，最具代表性的有GFS(Global
File System)、GPFS(General Parallel File
System)等。伴随着NAS^\[19\]^和SAN^\[20\]^技术的日益发展，研究人员考虑将两种体系结构结合起来，充分利用两者的优势。下面，对NAS和SAN两种技术进行分析和对比。

表1-1 NAS和SAN的分析与对比

+--------------------------+--------------------+--------------------+
| 存储系统架构             | NAS                | SAN                |
+==========================+====================+====================+
| 安装难易度               | 易                 | 难                 |
+--------------------------+--------------------+--------------------+
| 数据传输协议             | TCP/IP             | FC                 |
+--------------------------+--------------------+--------------------+
| 传输对象                 | 文件               | 数据块             |
+--------------------------+--------------------+--------------------+
| 是否使用标准文件共享协议 | 是                 | 否                 |
+--------------------------+--------------------+--------------------+
| 异种操作系统文件共享     | 是                 | 需要转换设备       |
+--------------------------+--------------------+--------------------+
| 集中式管理               | 是                 | 需要管理工具       |
+--------------------------+--------------------+--------------------+
| 管理难易度               | 易                 | 难                 |
+--------------------------+--------------------+--------------------+
| 是否能提高服务器效率     | 是                 | 是                 |
+--------------------------+--------------------+--------------------+
| 故障忍受度               | 高                 | 高（使用专有方案） |
+--------------------------+--------------------+--------------------+
| 适用对象                 | 中小型企业         | 大型企业           |
+--------------------------+--------------------+--------------------+
| 应用环境                 | 局域网             | 光纤通道区域网络   |
|                          |                    |                    |
|                          | 文档共享程度高     | 网络环境复杂       |
|                          |                    |                    |
|                          | 异质格式存储需求高 | 文档共享程度高     |
|                          |                    |                    |
|                          |                    | 服务器数量多       |
+--------------------------+--------------------+--------------------+
| 业务模式                 | WEB服务器          | 大型资料库         |
|                          |                    |                    |
|                          | 多媒体资料存储     | 数据库等           |
|                          |                    |                    |
|                          | 文件资料共享       |                    |
+--------------------------+--------------------+--------------------+
| 档案格式复杂度           | 中                 | 高                 |
+--------------------------+--------------------+--------------------+
| 容量扩充能力             | 中                 | 高                 |
+--------------------------+--------------------+--------------------+

不同领域内的企业应用对存储系统提出了各种不同的要求，如大容量、高可用性、高性能、可管理性、可扩展性、按需服务等。存储系统渐渐成为业界现阶段的研究热点，它作为云计算的基础，系统性能的好坏将直接影响到上层云计算相关应用的服务质量。业界内已推出各式各样的分布式存储系统并投入使用，如GFS(Google
File System)^\[21\]\[22\]^、HDFS(Hadoop Distributed File
System)^\[23\]^、PVFS(Parallel Virtual File
System)^\[24\]^、Lustre^\[25\]^、Ceph^\[26\]^等，每一种分布式存储系统都面向不同应用场景和特殊需求。

对于元数据管理模块，不同的分布式存储系统都有着各自不同的设计理念。HDFS和GFS基本同构，对于系统中的所有元数据信息，两者都采用单一节点进行管理，并将它们存储在内存中。对应于这种架构，研究者们都采用了很多的优化措施，如配置影子服务器来保证系统可用，利用超大文件块来减少元数据总量等，但这样的架构在扩展性和通用性方面必然存在问题。PVFS和Lustre主要面向高性能计算的应用场景，通过硬件来对系统可用性进行保证。其同样采用单节点的元数据管理方式，因此扩展性方面也存在瓶颈。Panasas采用了分布式的元数据管理策略，这种保证需要专门的硬件进行支持，不适合通用的应用场景。Ceph则在通用电脑上利用动态子树划分的方法实现了分布式元数据管理策略，将元数据命名空间按不同子树在不同的元数据管理节点上进行管理。但这种方式在命名空间改变时会造成大量的数据迁移，同时，在获取较深层次元数据信息时网络交互的代价太大。

## 1.3 本文研究内容

本文对现在流行的分布式存储系统的元数据管理模型进行充分的调研和分析，设计并实现了一种高可用性、高可靠性、高并发性、容错性的分布式存储系统元数据管理技术，研究内容主要分为以下三个方面：

（1）概述了现有的分布式存储技术，将目前分布式存储系统中的元数据管理模型分成集中式元数据管理模型、分布式元数据管理模型和无元数据管理模型三种，介绍其原理及缺陷，同时给出分布式存储系统中两种容错机制的简介，并分析它们各自的优缺点。

（2）研究了一种面向运营商领域、具有高可靠性和高可扩展性的分布式存储系统ZettastorDBS，用来解决海量数据的存储问题。该系统架构由两部分组成：数据通路模块和控制通路模块，同时将系统中的数据流和控制流分开。在本文提出的架构中，用户访问的控制流不再经过元数据服务器，有效避免了单点故障问题。

（3）实现了ZettastorDBS系统中的元数据管理方案。研究系统中元数据的组织和管理形式，采取在存储节点上将数据信息和元数据信息同时冗余存储的方式，来提高系统的可靠性。给出元数据的组织形式和元数据库中表结构的设计，研究元数据的各个状态以及各个状态之间的迁移实现过程，还有元数据管理技术中的核心程序。同时，在Linux环境下部署整个系统并对其进行功能性测试。

## 1.4 本文组织结构

本论文阐述了一种分布式存储系统中的元数据管理技术的设计与实现，主要结构安排如下：

第一章为绪论部分，介绍本论文的背景与意义、国内外研究现状及主要工作。

第二章概述了分布式存储系统中的概念及其应用，对当前主要的三种元数据管理模型进行了分析，并介绍了两种分布式存储系统中的容错机制。

第三章分析了本课题ZettaStor
DBS存储系统及其元数据管理部分的需求，给出了ZettaStor
DBS存储系统的总体架构，并按照元数据管理部分的需求给出了相关技术要点和基本流程设计。

第四章元数据管理功能的实现，首先介绍了元数据库的设计与实现，然后给出元数据状态的设计与迁移的实现过程，最后详述了元数据管理程序的设计与实现过程。

第五章对元数据管理模块进行了系统测试，介绍测试环境和方法。描述了测试过程，并对测试结果进行展示。

第六章是对全文工作进行的一个总结，指出元数据管理部分存在的问题，并对今后的完善方向进行了展望。

# 第2章 元数据管理技术概述

## 2.1分布式存储系统

### 2.1.1分布式存储系统简介

李凯于1986年在国内最早提出分布式共享存储（DSM，Distributed Shared
Memory）的概念^\[27\]^，他的设计有受到传统的虚拟存储系统的影响。虚拟存储系统中，当访问页面不存在于实存页面时，就会产生页面中断，操作系统会将这页面从硬盘中拿出再存到实存里。李凯的设计与以上过程相似，不同的在于他不是从实存中取出所需要的页面，而是从网络中的其他存储器中获得。在分布式存储系统中，物理上不属于同一个计算机的存储空间，逻辑上可以构成一个完整的虚拟存储空间。

随着信息技术的迅猛发展，计算机所需处理的信息量在急剧增长，互联网的出现带来计算机存储需求和存储环境的极大变化，分布式存储技术也正在发生着巨大的改变，主要变化表现在以下4个方面：

（1）数据分布在地域空间上更加分散^\[28\]^。随着互联网的发展，全球范围内任一区域的计算机都可以连接到网络中，这使得用户可以访问全球范围内的任意数据。

（2）数据存储量巨大。互联网技术日益发达，网络上的数据量呈爆炸式增长^\[29\]^，这就要求现阶段的存储系统应该具有巨大的存储容量^\[30\]^，还有一点更重要的是，系统应该具有灵活的可扩展性能。

（3）数据可靠性的要求增高。存储系统在实际应用时^\[31\]^，对于企业来说数据资源比硬件设施更宝贵，尤其是金融、医疗、军事等敏感的行业，对于数据的安全性要求更高。

（4）支持的应用更加广泛。连入互联网的节点数量和接入设备种类都越来越多，高效管理各种计算设备所产生的海量数据，同时有效使用这些数据成为一个新挑战。

在这种环境中，传统存储技术和客户机/服务器的存储模式远远不能满足时代需要。在应用需求的推动下，分布式存储系统正在快速发展。分布式存储系统是通过Internet将大量普通PC服务器互联作为一个整体对外提供存储服务的系统。一个合格先进的分布式存储系统须具有以下8个特性：

（1）高性能，衡量分布式存储系统性能常见的指标是系统的吞吐量和系统的响应延迟。系统的吞吐量是在一段时间内可以处理的请求总数，可以用QPS（Query
Per Second）和TPS（Transaction Per
second）衡量，系统的响应延迟是指某个请求发出到接收到返回结果所消耗的时间，通常用平均延迟来衡量。这两个指标往往是矛盾的，追求高吞吐量，比较难做到低延迟，追求低延迟，吞吐量会受影响。每一个用户访问系统时都要尽量减少网络的延迟和因网络拥塞、网络断开、节点故障等问题造成的影响；

（2）高可靠性，重点指分布式系统数据安全方面的指标，数据可靠不丢失，主要用多机冗余、单机磁盘RAID等措施。大多数系统的设计者都会考虑高可靠性这个问题。分布式环境一般都会有高可靠性的需求，用户将文件保存在分布式存储系统中的一个最基本要去就是数据可靠。

（3）高扩展性，指分布式存储系统通过扩展集群服务器规模从而提高系统存储容量、计算和性能的能力。业务量增大，对底层分布式存储系统的性能要求越来越高，自动增加服务器来提升服务能力，分为Scale
Up与Scale
Out，前者通过增加和升级服务器硬件，后者通过增加服务器数量。高可扩展性要求集群具有线性的可扩展性，系统整体性能与服务器数量呈线性关系。节点规模和数据规模会随着用户需求而扩大，这就要求分布式存储系统具有良好的高扩展性。

（4）透明性，网络中存储节点之间的物理关系对用户透明，用户在访问任一节点中数据时都像是访问本地数据一样。

（5）自治性，分布式存储系统应具有一定的自我恢复和维护功能。

（6）数据一致性，分布式存储系统多个副本之间的数据一致性，有强一致性，弱一致性，最终一致性，因果一致性，顺序一致性。

（7）高安全性，指分布式存储系统不受恶意访问和攻击，保护存储数据不被窃取，互联网是开放的，任何人在任何时间任何地点通过任何方式都可以访问网站，针对现存的和潜在的各种攻击与窃取手段，要有相应的应对方案。

（8）高稳定性，这是一个综合指标，考核分布式存储系统的整体健壮性，任何异常，系统都能坦然面对，系统稳定性越高越好。

### 2.1.2分布式存储系统应用

本节将主要介绍分布式存储系统在互联网和运营商领域内的应用。

**2.1.2.1分布式存储系统在互联网中的应用**

随着移动互联网、社交网络、电子商务等相关技术的日益发展，客户在互联网的使用过程中产生了海量的数据。为了有效地处理这些数据，每个互联网公司在其后端都有一套成熟的分布式系统用于数据的存储、计算以及价值提取。作为全球最大的互联网公司，Google公司在分布式技术上相对成熟，它公布的分布式文件系统GFS(Google
File
System)、分布式计算系统MapReduce、分布式表格系统Bigtable都成为业界争相模仿的对象，其公布的全球数据库Spanner更是能够支持分布在全世界各地上百个数据中心的上百万台服务器。Google的核心技术正是后端这些处理海量数据的分布式系统。和Google类似，国外的亚马逊、微软以及国内的互联网三巨头百度、阿里巴巴和腾讯的核心技术也是其后端的海量数据处理系统。

**2.1.2.2分布式存储系统在运营商中的应用**

针对运营商不同的业务类型，大致可以分为数据库、归档、虚拟化和大数据。从传统的硬件选择来看，这些业务系统通常采用带库设备或者磁盘阵列进行存储。随着分布式存储技术的日益发展，分布式块存储系统和分布式文件存储系统在运营商系统中已经逐渐投入使用。

1.资源池应用

有着诸多特性的分布式存储系统所有，在运营商资源池系统中的应用非常广泛。一方面资源池对于存储类型的需求更加多样化，基于卷方式访问的块存储设备，又或是基于文件方式访问的文件存储设备，分布式存储系统都可以很好的满足。另一方面，未来资源池系统规模会随着业务的增长而扩大。横向扩展是分布式存储系统中一个非常重要的技术特点，它可以很好的满足这一需求。

在分布式存储系统中，数据一致性需要很强大的内部交换系统来实现，同时对于分布式系统内部数据存储算法的要求非常高。而在资源池环境中，数据存储的性能需求不比对核心业务系统中的OLTP类应用那样搞，这就减小了分布式系统在这方面的性能挑战及压力。

2.数据归档应用

针对运营来说，企业中大量的数据需要归档和备份。较为传统的做法是将物理带库进行承载，然而对传统带库来讲，在磁带维护、机械故障、单文件恢复和空间利用方面都有着许多弊端。例如，对于磁带库中的存储介质来说，由于其受到不同环境因素的影响而出现消磁、磁带被腐蚀等后果；机械臂是物理带库中的核心部件，极其容易出现机械故障；更严重的是，对于磁带备份的数据，若要恢复某一时间备份的某份文件，则需要通过备份数据定位、机械臂选带、数据定位倒带等复杂而又费时的过程。在分布式存储系统中，将采用磁盘作为存储介质。通过基于数据校验或者副本的方式来保护数据，这样可以有效避免单个磁盘由于机械故障而带来的数据丢失问题。在恢复单文件的时候，不再需要像磁带那样进行大量的机械操作，只要快速定位到备份文件所在的存储节点，进行快速有效的读取恢复操作。同时，基于磁盘为存储介质的分布式存储系统可以通过重复数据删除、间接配置等功能，来降低存储容量的有效使用率。

3.公有云应用

通过互联网为企业或者个人提供云服务的平台称为云平台。系统若要构建在公有云上，则它所有的模块都需要具有水平扩展的能力。对个人企业和个人来说，公有云大多数时候呈现给客户的是企业云盘，通过这一载体为企业或个人来保存海量的非结构化数据。如图片、声音、影响等文件。同时，在公有云存储系统中，根据不同客户对存储要求的不同，可以从物理上来进行分区划分。分布式存储系统的分区特性，可以把不同性能、类型的存储分区资源整合分配给客户来使用。

4.测试业务应用

分布式存储系统具有便捷搭建、低成本等传统磁盘阵列所不具备的优势。运营商业务中会有许多不同的业务系统测试环境，对于这些应用，通过分布式存储系统的快速构建，可以在短时间内用低成本为运营商企业来搭建一套适用的测试环境。同时，通过分布式存储系统软件可以用大量现有的PC服务器，进行个性化的适用于特定场景的存储系统的搭建，可以实现有效的资源保护。

## 2.2元数据管理模型

衡量存储系统性能最主要的有三个指标：可靠性、存储速度和可用性。对于分布式系统而言，元数据处理是决定系统扩展性、性能以及稳定性的关键。因而，元数据管理模型显得至关重要。现有的分布式存储系统元数据管理方法主要有三类：集中式元数据管理模型、分布式元数据管理模型、和无元数据管理模型。前两类模型都是由元数据服务器来记录保存数据逻辑名字与物理信息的映射关系，包含文件访问控制所需要的所有元数据。对文件进行访问时，先向元数据服务器请求查询对应的元数据，对数据进行正确定位，然后通过获得的元数据进行后续的文件读写等I/O操作。第三类模型采用无元数据服务器的设计，取而代之使用算法来定位文件，元数据和数据没有分离而是一起存储。集群中所有存储系统服务器都可以智能地对文件数据分片进行定位，仅根据文件名和路径运用算法即可，而不需要查询索引或者其他服务器，这样可以完全实现并行化数据访问。

### 2.2.1集中式元数据管理模型

集中式元数据管理模型中，由专门的元数据服务器来记录保存所有的元数据，并处理客户端IO访问的寻址；通常基于高可用集群实现元数据服务器的高可用性及可靠性保护，如图2-1所示。出于简化系统设计复杂性的考虑，并且由于大量的历史遗留系统等原因，大多数分布式存储系统采用集中式的元数据服务，如HDFS^\[32\]\[33\]\[34\]^、Lustre、GFS等。

![](media/image3.jpeg){width="3.28125in" height="2.77578302712161in"}

图2-1 集中式元数据管理模型

集中式元数据管理模型虽然设计实现简单，极大提升了系统扩展性和性能，但存在两个关键的缺点。第一，元数据服务器在负载不断增大时将很快成为整个性能的瓶颈。根据Amdahl定律，系统性能加速比最终受制于串行部分的比重，而元数据服务器就是串行的部分，它直接决定着系统的扩展规模和性能。第二，比性能瓶颈更加严重的是单点故障。整个系统严重依赖于元数据服务器，一旦出现问题，系统将变得完全不可用，直接导致应用中断并影响业务连续性。

### 2.2.2分布式元数据管理模型

分布式元数据管理模型，实际上是集中式元数据管理模型的改进，元数据服务器的角色由一组基于分布式架构进行组织的服务器来承担，使得元数据处理能力、可靠性级别、以及系统可扩展性都有明显提升，如图2-2所示。这种模型又可以细分为两类，一为全对等模式，即集群中的每个元数据服务器是完全对等的，每个都可以独立对外提供元数据服务，然后集群内部进行元数据同步，保持数据一致性，如ISILON。另一类为全分布模式，集群中的每个元数据服务器负责部分元数据服务(分区可以重叠)，共同构成完整的元数据服务，如GPFS、Ceph。

![](media/image4.jpeg){width="4.677034120734908in" height="3.03125in"}

图2-2 分布式元数据管理模型

分布式元数据管理模型，将负载分散到多台服务器解决了性能瓶颈问题，利用对等的服务器或冗余元数据服务分区解决了单点故障问题。分布式看似非常完善，然而它大大增加了设计实现上的复杂性，同时可能会引入了两个新的问题。第一，分布式系统通常会由于节点之间的数据同步而引入额外开销，这是因为同步过程中需要使用各种锁和同步机制，以保证数据一致性。当元数据服务器规模较大时，高并发性元数据访问会导致同步性能开销更加显著。第二，数据一致性，这是分布式系统必须面对的难题。为了保证高可用性，元数据会被复制到多个节点位置，维护多个副本之间的同步具有很高的风险。如果元数据没有及时同步或者遭受意外破坏，同一个文件的元数据就会出现不一致，从而导致访问文件数据的不一致，直接影响到上层数据应用的正确性。这种风险发生的概率随着系统规模的扩大而大幅增加，因此分布式元数据的同步和并发访问是个巨大的挑战。

### 2.2.3无元数据管理模型

无元数据管理模型中，用户数据经切片后在各个存储节点上分布式存储，分布规则基于Hash算法生成，如图2-3所示。客户端在进行IO访问时，根据所请求的逻辑设备的相关寻址信息（例如LUN
ID和LBA
ID等）进行Hash计算，根据得到的Hash值去查询预先生成的对照表，从而获得IO访问的物理寻址信息，之后再将IO请求提交到对应的存储节点进行处理。目前，基于无元数据服务模型的分布式存储系统可谓凤毛麟角，Glusterfs^\[35\]^是其中最为典型的代表。

![](media/image5.jpeg){width="3.3229166666666665in"
height="3.2798742344706913in"}

图2-3 无元数据管理模型

无元数据模型虽然没有单点故障和性能瓶颈问题，但数据一致问题更加复杂，文件目录遍历操作效率低下，缺乏全局监控管理功能。同时也导致客户端承担了更多的职能，比如文件定位、名字空间缓存、逻辑卷视图维护等等，这些都增加了客户端的负载，占用相当的CPU和内存。

## 2.3分布式存储系统中的容错机制

容错机制^\[40\]^指的是系统由于某些原因出现数据丢失、任务失败、文件损坏、节点宕机等情况，系统能够有效恢复到原来状态并提供正常服务的一种机制。举个简单例子，我们在使用电脑的某个程序时，常常会遇到"程序无反应"或"程序未响应"的情况发生，此时这个程序便不能在进行下去，但经常会在过了几秒钟后恢复到正常使用的状态。这种"无反应"或"未响应"几秒钟的错误状态，我们便称之为"容错"。

### 2.3.1 Master/Slave容错机制

Master/Slave架构如图2-4所示，Master和Slave上同时存储系统的元数据，并且都对Client提供服务。区别在于，Master对元数据提供读写操作，Slave对元数据提供只读操作。

当Client需要获取系统的元数据时，向Master或者Slave发起read操作的请求；当Client需要对系统的元数据进行更改时，只能向Master发起write操作的请求。系统会将Master上的元数据信息定时同步到Slave上，这样当Master出现故障不能工作时，Slave上仍然保存着完整的元数据信息。所以在Master故![](media/image6.png){width="3.667361111111111in"
height="2.6881944444444446in"}障恢复之前，系统对外只能提供只读服务。

图2-4 Master/slave架构

优点：架构简单，易于实现。

缺点：系统存在单点故障^\[41\]^，当Master出现故障时，系统对外只能提供只读服务，不能提供写服务。同时，元数据从Master同步到Slave的过程不是实时的。故在Master故障时，系统可能会丢失最近一部分的元数据信息。

### 2.3.2 Multi-Master容错机制

![](media/image7.png){width="4.881944444444445in"
height="2.8020833333333335in"}Multi-Master是指系统中存在多个Master，每一个Master对Client都提供读写操作。Client可以向任意一个Master发起读写操作的请求，如图2-5所示。因为系统中存在多个Master，故必然存在数据一致性问题。多个Master之间需要使用相关协议来进行数据同步。

图2-5 Multi-Master架构

优点：有效解决单点故障。

缺点：数据一致性实现比较复杂。

## 2.4本章小结

本章首先概述了分布式存储系统的概念，并介绍了分布式存储系统在各个行业领域内的应用，尤其是互联网和运营商领域；接着介绍了当前众多分布式存储系统所运用的三种元数据管理模型，并对其进行分析与总结；最后介绍了两种常见的容错机制，并对其进行分析与总结。

# 第3章 分布式系统元数据管理方案设计

## 3.1需求分析

ZettaStor
DBS分布式存储系统来源于南京鹏云网络科技有限公司的项目，该存储系统的主要功能是对海量的通信数据进行分布式存储。元数据管理是ZettaStor
DBS分布式存储系统中的一个模块，本小节将结合ZettaStor
DBS分布式存储系统的总体需求，对它的元数据管理部分进行需求分析。

### 3.1.1分布式存储系统总体需求

![](media/image8.jpeg){width="4.072916666666667in"
height="2.5729166666666665in"}ZettastorDBS分布式存储系统的主要功能是对海量的用户通信数据进行分布式存储，根据功能可以分解为客户端访问部门、元数据管理部分、数据存储部分和系统调度部分，如图3-1所示。客户端访问部分是客户使用ZettastorDBS分布式存储系统是调用的接口，当客户使用这些接口时，ZettastorDBS系统其他部分对用户来说是透明的。元数据管理部分存储着ZettastorDBS系统的元数据信息，为系统内部各个模块提供元数据访问接口。用户在使用系统时，会先使用元数据管理部分来进行寻址定位操作，故元数据管理部分直接影响系统的性能。数据存储部分负责将通信数据存储在磁盘上，需要支持很高的并发性。系统调度部分监控系统数据存储部分的负载信息，并需要根据一定的策略对存储部分的数据进行迁移，来保证存储部分的稳定性。

图3-1 ZettastorDBS系统功能图

综上所述，ZettastorDBS分布式存储系统暂时主要用于通信行业，它的需求决定了系统必须具有高可用性、高可靠性、高IO性能和高可扩展性。

### 3.1.2元数据管理需求

**3.1.2.1功能性需求**

元数据管理部分管理ZettaStor
DBS分布式存储系统的元数据，其提供对元数据的创建、删除、查询、更新功能，如表3-1所示：

表3-1 用户功能需求

  功能         功能概述
  ------------ ----------------------------------------------------------------------------------------------------
  创建元数据   在元数据管理模块的数据库的相应表格中创建并插入保存元数据。其中克隆元数据本质上也属于创建一个新卷。
  删除元数据   根据范围删除元数据管理模块中指定的元数据记录。
  查询元数据   根据用户给定的条件查询某个范围内的所有元数据记录。
  更新元数据   根据范围更新元数据管理模块中指定的元数据记录，其中核心的需求主要是扩展元数据。

![](media/image9.jpeg){width="5.15625in"
height="2.9791666666666665in"}根据元数据管理的功能列表，用户功能用例图如图3-2所示：

图3-2 用户功能用例图

**3.1.2.2非功能性需求**

元数据管理模块除了功能性需求外，整体还应该具备可用性、可靠性、高并发性、容错性等特性。

1.可用性

用户在使用ZettaStor
DBS分布式存储系统时，能够随时随地准确访问到自己需要的数据^\[36\]^。这就要求整个存储系统具有很好的可用性，特别是元数据管理部分。因为系统需要元数据管理部分准确寻址定位到相应的数据存储服务器，才能做数据的相应操作。

2.可靠性

用户在存储系统中发起的每一次读写请求都会涉及到很多元数据信息部分的索引。那么只要元数据部分的信息出现存储错误或者信息丢失等问题^\[37\]^，就回导致大面积的数据无法访问甚至整个存储系统的瘫痪。所以，元数据信息的管理部分在整个存储系统中就显得非常重要，我们需要很好的保证存储系统中元数据管理部分的准确性和可靠性。

3.高并发性

在ZettaStor
DBS分布式存储系统中，用户每次对存储服务器中的通信数据进行操作，都必须首先通过元数据管理部分获取到对应的元数据信息，进行寻址定位操作。所以，ZettaStor
DBS分布式存储系统要能响应海量并发用户请求，则需要系统的元数据管理部分也能够响应海量并发请求^\[38\]^，不然元数据管理部分就会成为ZettaStor
DBS分布式存储系统的一个很大的性能瓶颈。

4.容错性

元数据管理部分存储着ZettaStor
DBS分布式存储系统中的元数据信息，若元数据服务器出现故障造成数据丢失，则ZettaStor
DBS分布式存储系统将无法正常的为用户提供服务。所以，要求元数据管理部分在对元数据信息存储管理时要有很好的容错性^\[39\]^，这样在系统发生某些故障时能够对元数据进行及时有效地恢复。

## 3.2分布式存储系统结构

### 3.2.1总体架构

本文中的ZettaStor
DBS分布式存储系统是一种典型的软件定义分布式存储产品，面向私有云及企业级数据中心环境，并广泛支持各类典型应用场景。即可独立部署为企业级Server
SAN存储平台，也可与业务系统混合部署为超融合系统。基于该产品构建的存储系统能够提供更为敏捷、智能、高效、可靠的IT基础架构，并显著降低总体拥有成本，助力用户成功应对新的挑战，把握市场机遇。

![](media/image10.jpeg){width="5.635416666666667in"
height="3.1041666666666665in"}图3-3 系统总体架构

ZettaStor
DBS系统由一系列承担不同任务的软件模块构成，通过模块之间的通信与数据传递来实现系统的各项功能。如图3-3所示，这些模块可以分成数据通路相关和控制通路相关两类。数据通路相关模块负责管理底层存储资源，建立并管理IO通路，接受用户数据访问请求并完成数据的读写操作；控制通路相关模块与用户数据访问过程隔离，负责收集并呈现系统内各类信息，提供用户管理界面及接口，以及完成系统各项管理任务。

### 3.2.2数据通路模块

如图3-4所示，数据通路模块包括客户端驱动、Coordinator、DriverContainer和DataNode。

![](media/image11.jpeg){width="5.768055555555556in"
height="3.8430555555555554in"}图3-4 数据通路模块及访问关系

1.客户端驱动

客户端驱动负责与Coordinator进行通讯，来识别由系统所提供的标准块存储设备，并建立数据访问通路。客户端驱动还将所识别到的块设备提交给操作系统进行读写访问，将接收到的用户IO请求提交给Coordinator，并返回获取的执行结果。

2.DriverContainer及Coordinator

Coordinator把一系列分布在多个存储节点服务器上的存储空间组织起来，就构成了分布式存储系统所提供的数据卷。Coordinator作为服务端来响应客户端驱动所提交的IO请求并进行寻址，再将IO请求分发给对应的存储节点服务器上的DataNode进程，并向客户端驱动返回处理结果。

每个Coordinator负责管理一个标准块存储设备。DriverContainer维护所在存储节点服务器上的多个Coordinator进程，以提供多个块设备。

3.DataNode

DataNode模块部署在每个每台存储节点服务器上，是物理存储资源的实际管理者，以及数据读写操作的实际执行着。DataNode模块承担以下任务：

（1）识别并管理所在存储节点服务器内部的硬盘，格式化并管理每块硬盘上存储空间的使用；

（2）接收Coordinator分发的读写请求，进行数据读写操作；

（3）维护本地数据与其他DataNode上数据之间的副本关系，并在故障时执行主从副本切换、数据重构等操作。

### 3.2.3控制通路模块

![](media/image12.jpeg){width="4.385416666666667in"
height="3.7291666666666665in"}控制通路模块及数据流基本构成如图3-5所示，其中各模块均与数据通路分离，模块发生故障仅影响管理类操作，而不会影响数据的正常读写访问。为避免控制通路相关模块故障给系统管理带来不便，默认这些模块都采用HA方式部署在不同的两个节点上，相互保护，当其中一个节点故障，另一个可以快速接管工作。

图3-5 控制通路模块及基本控制流构成

1.InfoCenter

InfoCenter是系统的信息中心，负责从DataNode和DriverContainer获取包括DataNode和后端存储资源信息、块设备挂载和使用信息、及账户信息在内的各类信息，供ControlCenter及Console使用。

2.ControlCenter

ControlCenter是系统的控制中心，各类系统管理操作请求，如块设备的创建、挂载、删除等，都需要通过ControlCenter提交并调用相应的模块完成处理。

3.Console

Console为系统的控制台，用户可以通过web浏览器方式进行系统配置、查看各类系统信息，包括各部件运行状态、各类故障或告警、以及IO性能数据
和报告等操作。

## 3.3元数据管理基本流程

由元数据管理部分的需求可得元数据基本操作包括：创建元数据、更新元数据、删除元数据、查询元数据。

1.创建和更改元数据

![](media/image13.jpeg){width="3.84375in"
height="2.261111111111111in"}创建元数据和更改元数据流程相同，如图3-6所示。由用户触发，Console软件模块发起request给控制中心ControlCenter，再由ControlCenter将接收到console发来的request处理成DataNode需要的请求格式发送给DataNode。相应的DataNode接收到请求后作创建或者更改处理，在操作完成之后把reponse按照原路返回。

图3-6 创建和更改元数据流程

2.删除元数据

![](media/image14.jpeg){width="4.895833333333333in"
height="2.3541666666666665in"}删除元数据的流程与创建元数据相比较，中间多了一个InfoCenter模块，如图3-7所示。因为InfoCenter中存储系统中所有的元数据信息，在DataNode将相应信息删除完之后，需要将InfoCenter中相应元数据信息删除掉，保持系统整体的元数据一致。

图3-7 删除元数据流程

3.查询元数据

![](media/image15.jpeg){width="2.9375in"
height="2.2465277777777777in"}查询元数据流程较简单，因为系统中所有的元数据信息存储在InfoCenter中，故在用户发起查询操作时，Console模块将请求直接发给InfoCenter，InfoCenter将响应和查询结果直接告知给Console即可。如图3-8所示。

图3-8 查询元数据流程

## 3.4本章小结

本章首先分析了系统整体需求和元数据管理部分需求。其次，根据系统需求描述了ZettaStor
DBS系统的总体架构，架构中包含两个模块：数据通路模块和控制通路模块。数据通路模块包括客户端驱动、Coordinator、DriverContainer和DataNode，控制通路模块包括信息中心InfoCenter、控制中心ControlCenter和控制台Console。最后，根据系统元数据管理需求给出元数据管理基本流程所包含的内容，主要是元数据的增删查改。下一章将围绕本章内容进行详细描述。

# 第4章 分布式系统元数据管理的功能实现

## 4.1元数据库设计与实现

### 4.1.1相关术语解释

本文中多次出现一些ZettastorDBS系统自定义概念或分布式存储技术中相关概念，为更好阅读和理解本文，现给出相关术语解释，如表4-1所示。

表4-1 相关术语解释

+-----------------+---------------------------------------------------+
| DataNode        | ZettastorDBS软件模块之一                          |
|                 | 。负责存储节点服务器及其存储资源的管理，并执行数  |
|                 | 据读写操作。有时也用DataNode指代存储节点服务器。  |
+=================+===================================================+
| Coordinator     | Zettast                                           |
|                 | orDBS软件模块之一。负责组织存储资源并对外发布，接 |
|                 | 收客户端驱动程序的IO请求，并分发给DataNode执行。  |
+-----------------+---------------------------------------------------+
| DriverContainer | Zettas                                            |
|                 | torDBS软件模块之一。管理本机上的Coordinator进程。 |
+-----------------+---------------------------------------------------+
| InfoCenter      | Zettas                                            |
|                 | torDBS软件模块之一。负责从DataNode和DriverContain |
|                 | er收集各类信息，提供给ControlCenter等其他软件模块 |
|                 | 使用，用于系统管理、维护、配置变更、资源调配等。  |
+-----------------+---------------------------------------------------+
| ControlCenter   | ZettastorDBS软件模块之一。负责从Console接收用户指 |
|                 | 令，并发布到DataNode和DriverContainer等模块，实现 |
|                 | 具体的系统管理、维护、配置变更、资源调配等操作。  |
+-----------------+---------------------------------------------------+
| Console         | 控制台，ZettastorDBS软件模块之一。用户可以        |
|                 | 通过Web浏览器直接访问，对存储资源进行配置和管理。 |
+-----------------+---------------------------------------------------+
| 组              | 即Group。ZettastorDBS内部针对存储节点服务器的组   |
|                 | 织形式，以保证数据可靠性和系统可用性为主要目的。  |
+-----------------+---------------------------------------------------+
| 域              | 即Domain。ZettastorDBS内部针对存                  |
|                 | 储节点服务器的组织形式，以让资源的组织更加贴近用  |
|                 | 户需求为目的，例如租户之间物理资源和故障的隔离。  |
+-----------------+---------------------------------------------------+
| 存储池          | 即Storage                                         |
|                 | Pool。Zetta                                       |
|                 | storDBS内部针对硬盘等存储资源的组织形式，可按介质 |
|                 | 类型划分不同的存储池，以适应不同类型应用的需求。  |
+-----------------+---------------------------------------------------+
| Archive         | ZettastorDBS内部将识别                            |
|                 | 到并完成初始化的硬盘称为Archive，包括磁盘和SSD。  |
+-----------------+---------------------------------------------------+
| SegmentUnit     | ZettastorDBS将硬盘划分成指定大小的Segment         |
|                 | Unit，并以Segment Unit为单位进行存储空间分配。    |
+-----------------+---------------------------------------------------+
| Segment         | 多个Segment                                       |
|                 | Unit互为副本构成一个Segme                         |
|                 | nt。再由一组Segment构成一个数据卷，供客户端访问。 |
+-----------------+---------------------------------------------------+
| 主副本          |   --------------------------------                |
|                 | ------------------------------------------------- |
|                 | ------------------------------------------------- |
|                 |   也称为Primary。构成Segment的数个                |
|                 | Segment Unit之一。执行数据写入操作时必须保证主副  |
|                 | 本写入成功，执行数据读取操作时由主副本进行响应。  |
|                 |   --------------------------------                |
|                 | ------------------------------------------------- |
|                 | ------------------------------------------------- |
+-----------------+---------------------------------------------------+
| 从副本          | 也称为Secondary。构成Segment的其他非主副本Segment |
|                 | Unit。在主副本失效后，                            |
|                 | 各个从副本会从中自主选举出新的主副本，并接管作。  |
+-----------------+---------------------------------------------------+
| 数据卷          | 即Volum                                           |
|                 | e。ZettastorDBS以数据卷的形式将存储资源提供给客户 |
|                 | 端主机使用，客户端使用数据卷，如同使用本地磁盘。  |
+-----------------+---------------------------------------------------+
| 元数据          | 即Metadata。Archive、Segment                      |
|                 | Unit、数据卷等对象                                |
|                 | 都有自己的元数据，记录自身的数据格式及组织信息。  |
+-----------------+---------------------------------------------------+

### 4.1.2元数据组织形式

**4.1.2.1元数据整体组织关系**

系统中，将三个或三个以上的存储节点组成一个域domain，由来自至少三个存储节点服务器上的磁盘组成一个存储池storagePool，一个域domain可以包含多个存储池storagePool。域domain和存储池storagePool创建好后，用户可以在storagePool中创建多个卷volume，卷是系统提供给用户存取数据的逻辑操作单元，它由多个固定存储空间大小的segment组成。每个segment又是由数个segmentUnit构成，segmentUnit之间是主从副本的关系，内容完全一致。卷的元数据属性中有一个名为accountid的属性，只有当volume的accountid与用户的accountid相同时，用户才有权限对这个卷进行操作。图4-3可以反映出系统中卷volume和存储池storagePool的关系。以下将详述存储服务器中磁盘格式及元数据分布、卷的构成和卷空间在系统内的分布。

ZettastorDBS直接以裸设备的方式将存储节点服务器上的硬盘设备纳入管理，而无需本地文件系统或其它任何中间层次。在ZettastorDBS系统中，将所识别到并完成初始化操作的硬盘称为Archive，并将其存储空间划分成为一个个指定大小的SegmentUnit，作为执行存储空间分配的基本单元。ZettastorDBS管理下的Archive内部格式如图4-1所示。其中，位于硬盘起始位置的ArchiveMetadata负责记录自身存储空间的使用和SegmentUnit的分布信息。而在每一个SegmentUnit的起始位置，则是存放每一个segmentUnit的元数据信息，也有相应的Metadata记录自身相关信息。若该segmentUnit所属的segment在volume中逻辑位置排在第一时，则该segmentUnit的起始位置除了存储自身的Metadata以外，还存储它所属卷的volumeMetadata信息。

![](media/image16.jpeg){width="4.208333333333333in"
height="1.0729166666666667in"}图4-1 Archive格式及Metadata分布

![](media/image17.jpeg){width="4.729166666666667in"
height="2.293733595800525in"}如图4-2所示，每一个数据卷都是由一系列被称为Segment的逻辑单元所构成的，而每个Segment则是由数个SegmentUnit构成的。如上所述，SegmentUnit是系统执行存储空间分配的基本单元，是某块硬盘上的一段连续的存储空间。每个SegmentUnit就是所属Segment的一个数据副本，有几个副本就有几个SegmentUnit。这些副本之间数据保持一致，而且一定来自不同的存储节点服务器。这样，任何一个硬盘故障或存储节点服务器故障，受到影响的只是某一个数据副本，数据的可靠性和可用性就得到了保障。ZettastorDBS系统针对数据卷来设置数据副本个数。图4-2所示为一个三副本的数据卷，则其中每个Segment均由三个SegmentUnit构成，其中一个作为主副本（Primary），另外两个作为从副本（Secondary）。

图4-2 Volume的构成

数据卷volume可支持两个副本（一个主副本和一个从副本），以及两个以上副本（一个主副本和两个或两个以上副本）的配置。数据副本的个数越多，能容忍的故障节点数就越多，数据卷的可靠性及可用性级别就越高。设置为两副本的数据卷能够容忍其中一个数据副本发生故障，用户数据仍可正常读写访问。设置为三个及以上副本的数据卷，可容忍数据副本陆续发生故障，直至只剩余一个数据副本可以使用，此过程中用户数据仍可正常读写访问。为了让空间利用率和性能表现达到最优，系统创建数据卷时，会尽量使数据卷的所有segmentUnit以及其中的主副本在各台存储节点服务器上均匀分布。图4-3是一个三副本配置的数据卷所属存储空间在系统内的分布示意图。

![](media/image18.jpeg){width="4.083333333333333in"
height="2.6727274715660543in"}图4-3 Volume空间在系统内的分布

**4.1.2.2根卷和扩展卷之间的关系**

系统存在这样的配置场景，在建立一个volume后，用户发现现有的空间不足以支持后续的需求，可以对该volume进行容量扩展。原有的volume为称为根卷rootVolume，扩展的volume为childVolume。一个volume可以扩展多次。

系统在内部实现上，每次扩展都产生一个childVolume，代表这个childVolume的对象volume
metadata的字段和rootVolume一致。但是用户在console上看不到对应的childVolume。ChildVolume包含的segment
units信息会合并在rootVolume。用户在console上看到的是childVolume和rootVolume合并后的虚拟的volume。ChildVolume同样有自己的volume
status，会影响rootVolume的状态，如：如果某个childVolume的状态为unavailable，
那么rootVolume同样为unavailable。在volume
metadata中有几个重要字段，用来维系volume的root和child的关系。它们分别是：

volumeId：ID表示，每个volume具有不一样的volumeId;

rootVolumeId:
指示rootVolume的id，如果自身为rootVolume，那么rootVolumeId和volumeId一致，如果为childVolume，指向rootVolume的volumeId。一个rootVolume下的所有childVolume的rootVolumeID都相同，指向同样一个rootVolumeId;

childVolumeId:
指示下一个childVolume。所有的childVolume通过该字段链接成一个单链表。

以rootVolume的volumeId为1，扩展3次childVolume为例，链接流程如下：

（1）初始状态：

（2）第一次扩展：

（3）第二次扩展：

（4）第三次扩展：

### 4.1.3元数据库表结构设计

上一小节详细讲述了系统中元数据的组织形式，这一小节将介绍元数据库中各个表结构的设计。

表4-2为卷volume的结构设计。由4.1.2可知，卷是系统提供给用户存取数据的逻辑操作单元，它由用户自己创建，包含多个固定大小的segment。其中，卷类型volumeType、卷缓存类型cacheType、卷状态volumeStatus、限制卷的读写速度的规则ioLimitations、卷创建方式volumeSource和卷的读写权限readWrite为系统自定义数据类型。卷的状态volumeStatus是卷的一个非常重要的属性，所有关于卷的操作都是基于volumeStatus来进行的。卷状态的具体设计与实现将在4.2小节中详细阐述。Volume中其他属性名称及其属性含义详见表4-2，这里不再赘述。

表4-2 卷volume

  属性名称                     属性类型               属性含义
  ---------------------------- ---------------------- --------------------------------------------------------------------------------------------------------------------------
  volumeId                     Long                   卷id号，主键。
  rootVolumeId                 Long                   根卷的id号。
  childVolumeId                Long                   子卷的id号。
  domainId                     Long                   所属域的id号。
  storagePoolId                Long                   所属存储池的id号。
  volumeSize                   Long                   卷大小，即包含的segment的个数。
  extendingSize                Long                   卷扩展的大小，即扩展了多少个segment。
  name                         String                 卷名称，由用户创建时的给定，也可后期更改。
  volumeType                   VolumeType             卷类型，系统自定义类型。
  cacheType                    CacheType              卷缓存类型，系统自定义类型。
  volumeStatus                 VolumeStatus           卷状态
  accountId                    Long                   卷所属的管理员id号
  segmentSize                  Long                   一个segment的大小，通常根据具体业务场景需要事先给定，也可以通过配置修改。
  deadTime                     Long                   卷从开始删除到删除成功所经过的时间
  volumeLayout                 String                 卷中segment编号的排列。如一个卷有5个segment，则排列为0到4。
  simpleConfigured             int                    标记这个卷是否为精简配置。精简配置卷是指系统不是一次性把用户申请的存储空间分配到位，而是随着用户实际数据量的增长来供给。
  segmentNumToCreateEachTime   int                    精简配置下，每次创建的segment数量。
  ioLimitations                List\<IOLimitation\>   一种用来限制卷的读写速度的规则。
  freeSpaceRatio               double                 卷剩余容量。
  volumeCreatedTime            Date                   卷创建的日期。
  lastExtendedTime             Date                   卷最近扩展的日期。
  lastUpdateTime               Date                   卷最近更新的日期。
  volumeSource                 VolumeSourceType       卷创建方式（创建、克隆），系统自定义类型。
  readWrite                    ReadWriteType          卷的读和写权限，系统自定义类型。

表4-3为存储池storagePool的结构设计。由4.1.2可知，一个存储池由来自至少三个存储节点服务器上的不同磁盘组成，这些信息由变量archivesInDataNode来记录，这是系统自定义的一个map类型。storagePool中包含的volume由变量volumeIds来记录，这是一个Set的有序集合。其他属性名称及其含义详见表4-3。

表4-3 存储池storagepool

  属性名称             属性类型                 属性含义
  -------------------- ------------------------ ------------------------------------------------------
  poolId               Long                     存储池id号，主键。
  domainId             Long                     存储池所属的域的id号。
  name                 String                   存储池名称，由用户命名。
  description          String                   存储池相关描述。
  archivesInDataNode   Multimap\<Long, Long\>   存储池中所含磁盘的id号以及磁盘所在的存储节点的id号。
  volumeIds            Set\<Long\>              存储池中所有卷的id号。
  lastUpdateTime       AtomicLong               存储池最近更新的时间。
  status               Status                   存储存储池状态
  usedRatio            AtomicDouble             已使用的存储空间占池中的总空间的比例。

表4-4为域domain的结构设计。由4.1.2可知，一个domain至少包含三个不同的存储节点，这些信息由变量dataNodes来记录，这是一个Set的有序集合。domain中包含的所有storagePool由变量storagePools来记录，这也是一个Set的有序集合。其他属性名称及其含义详见表4-4。

表4-4 域domain

  属性名称            属性类型      属性含义
  ------------------- ------------- ----------------------------
  domainId            Long          域id号，主键。
  domainName          String        域名，创建时由用户输入。
  domainDescription   String        域的描述，相当于一些备注。
  dataNodes           Set\<Long\>   域中所包含的存储节点。
  storagePools        Set\<Long\>   域中所包含的存储池。
  lastUpdateTime      AtomicLong    域最近更新的时间。
  status              Status        域的状态。

表4-5为磁盘archive的结构设计。ZettastorDBS系统中控制流和数据流分离，元数据服务器中存储着系统所有的元数据信息。用户会根据权限和需求从元数据服务器中只拿取自己需要的部分元数据信息，存储在自己驱动挂载的个人服务器上，之后的IO访问路径只通过个人服务器，这种访问方式有效减轻元数据服务器的负担，避免了分布式存储系统中的单点问题，故在挂载驱动时需要知道磁盘的信息，具体详见表4-5。

表4-5 磁盘archive

  属性名称       属性类型        属性含义
  -------------- --------------- ------------------------------
  archiveId      long            磁盘id号，主键。
  serialNumber   String          序列号
  instanceId     InstanceId      磁盘所在的datanode的id
  pageSize       int             页大小
  deviceName     String          设备名称
  status         ArchiveStatus   磁盘状态
  storageType    StorageType     存储类型，有SSD、SATA或SAS。
  version        int             版本号
  archiveType    ArchiveType     磁盘类型
  createdTime    long            磁盘创新时间
  updatedTime    long            磁盘更新时间
  updatedBy      String          更新用户
  createdBy      String          创建用户
  description    String          磁盘描述信息
  logicalSpace   long            可以存放元数据的剩余空间。

## 4.2元数据状态迁移设计与实现

### 4.2.1元数据状态设计

**4.2.1.1 SegmentUnit状态设计**

由于segmentunit和数据块共同存储在存储节点上，所以segmentunit状态由datanode自己管理。系统中segmentunit一共有7种状态，数据结构定义和各状态解释如下：

public enum SegmentUnitStatus{

Primary(1),

Secondary(2),

OFFLINING(3),

OFFLINED(4),

Deleting(5),

Deleted(6),

Broken(7),

}

（1）Primary：该数据块segmentUnit是主副本并且是可读/可写的。

（2）Secondary：该数据块segmentUnit是从副本并且是可读可写的。

（3）OFFLINING：该数据块segmentUnit正在离线。

（4）OFFLINED：该数据块segmentUnit已经离线。

（5）Deleting：该数据块正在被删除。有两种情况，一种情况是客户端发起删除地操作；还有一种情况是存储节点之间通信时，发现同属于一个segment的segmentUnit的版本比其他segmentUnit版本低，则自己发起删除操作。

（6）Deleted：segmentUnit已经被删除，磁盘应该回收这个空间；或者是磁盘损坏，则磁盘上的所有segmentUnit状态都为Deleted。

（7）Broken：该数据块segmentUnit已经被损坏。

**4.2.1.2 Segment状态设计**

segment状态由其包含的所有segmentunit状态计算得出，并且将直接影响其所属的volume的状态。系统中segment一共有7种状态，数据结构定义和各状态解释如下：

public static enum SegmentStatus {

Unavailable(1),

Deleting(2),

Dead(3),

Writable(4);

}

（1）Unavailable：segmentUnit刚被创建时或者有所损坏时，则segmentUnit所属的segment状态为Unavailable；

（2）Deleting：卷volume被删除时，其包含的segmentUnit为正在删除状态，即Deleting状态，此时segmentUnit所属的segment状态也为Deleting；

（3）Dead：segment包含的所有segmentUnit删除成功时即状态为Deleted时，该segment状态为Dead；

（4）Writable：segment创建成功后，可读可写的状态。

**4.2.1.3 volume状态设计**

卷volume的状态是卷的一个非常重要的属性，系统响应用户对于卷的操作都是根据卷的状态来确定。系统中卷一共有7种状态，数据结构定义和各状态解释如下：

public enum VolumeStatus {

ToBeCreated(1),

Creating(2),

Available(3),

Unavailable(4),

Deleting(5),

Deleted(6),

Dead(7);

}

（1）ToBeCreated：用户调用infocenter接口创建volume时处于这种状态，此时datanode还未上报任何segmentunit；

（2）Creating：处于ToBeCreated状态的volume只要有一个segmentunit上报其迁移到Creating状态，表示datanode已经收到了controlcenter的创建消息；另外infocenter宕机重启后，datanode上报segment
index =
0时，会创建volume，因为此时已经有了segmentunit，所以volume自出生日起状态就是Creating；

（3）Available：volume中所有的segment的状态都是Available，segment状态为available的充分必要条件是：segment包含1个primary和quorumSize（系统定义的segment副本数）-1个secondary；

（4）Unavailable：volume中有一个segment处于unavailable状态，该volume就处于这种状态；

（5）Deleting：用户在console界面上删除该volume，该状态如果用户需要，可以恢复成之前的状态；

（6）Deleted：所有的segmen都处在deleting状态，如用户需要，可以恢复成之前状态；

（7）Dead：只要有一个segment的状态处在deleted状态，表示该volume不可用，即为dead状态。

### 4.2.2元数据状态迁移实现

**4.2.2.1卷状态迁移实现**

卷的各状态之间的迁移和迁移条件如图4-4所示：

图4-4 卷状态转换图

下面对每一步迁移以及驱动过程作详细说明：

1\. ToBeCreatedDead

图4-5 ToBeCreated迁移至Dead

如图4-5所示，卷当前状态为ToBeCreated，用户在控制台console上点击删除按钮，对该卷发起删除操作，则infoCenter模块会将所有用户要删除的卷放在任务队列taskqueue中，再从本地数据库中进行删除。同时，volumeSweeper线程会对每个任务队列taskqueue中的每个卷记录它的起始删除时间点和消耗时间（用户会根据系统具体规模和具体应用场景来设定一个删除时间上线值），当删除过程超时的时候，系统会自动将该卷状态迁移为Deleting状态。此时只要卷中有一个segmentunit被删除成功，即segmentunit状态为deleted时，则该卷已死，卷状态成功迁移为Dead。

> 2\. ToBeCreatedCreating

图4-6 ToBeCreated迁移至Creating

如图4-6所示，卷当前状态为ToBeCreated，用户在控制台console创建一个卷的时候，这个卷的初始状态为ToBeCreated，并且将所有要创建的的卷放在任务队列taskqueue中。由于datanode会定期将自己节点上所有segmentunit元数据信息report给infoCenter，所以当volumeSweeper线程发现有一个segmentunit被创建好并被成功汇报给infoCenter时，infoCenter中数据库会将该卷的状态迁移为Creating。同时，volumeSweeper线程会检查该卷包含的所有segment的创建状态，只要有一个segment没有被创建成功，则该卷一直保持为Creating状态。

> 3\. CreatingAvailable

图4-7 Creating迁移至Available

如图4-7所示，卷当前状态为Creating，即该卷正在创建中。用户在控制台console上触发创建操作，随即控制台console将创建请求发送给控制中心controlCenter，控制中心controlCenter将创建请求作相应处理，将创建卷的请求转化为创建segment的请求再发送给存储节点datanode，datanode会在自己节点上创建相应的segmentunit，最后将创建的结果按照原路反馈回去。同时，datanode又会将自己节点上所有的segment元数据信息汇总成一个list的形式，定时汇报给infoCenter，在infoCenter中整理成卷的形式给用户使用。infoCenter在接收到一个新卷的segmentunit是，会将其置入任务队列，等到VolumeSweeper
线程确认新卷的所有segment状态是OK的时候，infoCenter数据库中新卷的状态成功迁移至Available。

> 4\. AvailableUnavailable

图4-8 Available迁移至Unavailable情况1

图4-9 Available迁移至Unavailable情况2

由图4-8、图4-9可知，有两种情况，会使得卷状态从Available迁移至Unavailable。第一种情况如图4-8所示，用户在做卷的查询操作时，VolumeSweeper会对查询操作计时，当对卷中某个segmentunit的查询操作超时的时候，VolumeSweeper会将此segmentunit置入一个任务队列，直至VolumeSweeper发现该segmentunit所在的segment的查询超时的时候，则在infoCenter数据库中将该卷状态迁移为Unavailable。第二种情况如图4-9所示，在所有datanode定期汇报上来的segmentunit元数据信息中，infoCenter一旦发现某个segmentunit状态为broken，则将其置入一个任务队列中，直至VolumeSweeper发现队列中segmentunit所在的segment状态为broken，则在infoCenter数据库中将该卷状态迁移为Unavailable。

> 5\. UnavailableAvailable

图4-10 Unavailable迁移至Available

如图4-10所示，卷状态从Unavailable迁移至Available的过程与卷状态卷状态从Available迁移至Unavailable过程的第二种情况正好相反。卷当前状态为Unavailable，则该卷包含的所有segmentunit中至少有一个状态不为OK。在所有datanode定期汇报上来的segmentunit元数据信息中，infoCenter一旦发现某个segmentunit状态为ok，则将其置入一个任务队列中，直至VolumeSweeper发现该卷中所有的segment状态为ok，则在infoCenter数据库中将该卷状态迁移为Available。

> 6\. CreatingDeleting
>
> AvailableDeleting
>
> UnavailableDeleting

卷当前状态为Creating、Available或Unavailable时，用户需要删除改卷时，都是在控制台console上触发删除操作，触发该卷的状态迁移。这三种状态迁移过程在infoCenter中有接口可以调用，系统直接将卷状态改为Deleting，而不需要再经过VolumeSweeper线程进行加工了。

> 7\. DeletingDeleted
>
> DeletingDead
>
> DeletedDead

在删除卷的过程中，卷状态无论是由Deleting迁移至Deleted或Dead，还是由Deleted迁移至Dead，都与Creating状态迁移相似，由datanode上报segmentunit信息，置入队列，然后由VolumeSweeper进行处理。

**4.2.2.2卷状态加工流程**

在大部分的状态迁移中，都有涉及到超时线程timeoutSweeper。infoCenter中的timeoutSweeper用来检测volume和volume中所含的segmentunit信息是否超时，timeoutSweeper定期检测超时的volume和segmentunit是否超时,
timeoutSweeper包含delayQueue，定期从delayQueue中获取即将超时的segmentunit，然后判断segmentunit是否超时。delayQueue的设计较为巧妙：每次从delayQueue中只会取出部分数据，这样会使得性能提升很多。delayQueue是优先级队列，在超时时间内所有元素将会出队。现在超时时间为90s，即在![](media/image19.jpeg){width="5.643055555555556in"
height="2.8041666666666667in"}90s内所有的segmentunit才会处理一次。

图4-11 volumeStatus加工流程

图4-11展示了系统中关于卷状态的加工，InfocenterRequester和Volume
Sweeper为两不同线程，volumeStore存放volume信息。InforcenterRequester通过提供RPC接口给Datanode使用，让其上报segmentunit状态信息，然后将信息更新到volumestore中。
Volume
Sweeper线程调度时，从volumeStore获取所有的volume信息，加工其状态。

## 4.3元数据管理程序设计与实现

上述两个小节详述讲明了元数据的组织形式和具体的元数据信息，本小节将着重讲述元数据管理时用到的两个定时器和元数据重构接口的设计与实现。

### 4.3.1TimeoutSweeper定时器

TimeoutSweeper定时器属于infoCenter软件模块，用来定期检查卷是否等待创建超时，是否处于Dead状态超时，是否有segmentUnit数据更新超时，是否最终删除domain和storagePool等。

TimeoutSweeper中有一个计时器runTimes，runTimes每经过5秒，TimeoutSweeper就回检查volume的状态。当volume正在被创建，即状态为ToBeCreated，若创建时间超过10秒时，则该卷会被记录下来，后端会再去检查volume中的segmentUnit创建是否有问题；当volume状态为Dead，且处于Dead状态超过10秒时，说明系统需要从数据库中强制将这个状态已经为Dead的卷删除掉。核心代码实现如下：

if (runTimes % 5 == 0) {

/\* we do not need so hurry to deal with volume status \*/

for (VolumeMetadata volume : volumes) {

if (volume.getVolumeStatus() == VolumeStatus.ToBeCreated) {

if (now - volume.getCreateStartTime() \>
InfoCenterConstants.getVolumeToBeCreatedTimeout() \* 1000l) {

logger.debug(\"the volume in toBeCreated status is timeout:

created time is {}, current time is {}, timout is {}\",

Utils.millsecondToString(volume.getCreateStartTime()),
Utils.millsecondToString(now),

InfoCenterConstants.getVolumeToBeCreatedTimeout());

this.volumeStatusStore.addVolumeToStore(volume);

}

} else if (volume.getVolumeStatus() == VolumeStatus.Dead &&
volume.getDeadTime() != 0) {

// check out the dead volume need to delete in DB

// although volume is dead, dead time may be zero. for timeout sweeper
may

//execute between set

// volume status and set dead time

if (now - volume.getDeadTime() \>
InfoCenterConstants.getTimeOfdeadVolumeToRemove() \* 1000l) {

logger.debug(\"the volume in dead status is longer than the threshold:
deadTime is {}, current time is {}, timeout is {}\",

Utils.millsecondToString(volume.getDeadTime()),
Utils.millsecondToString(now),

InfoCenterConstants.getTimeOfdeadVolumeToRemove());

this.volumeStatusStore.addVolumeToStore(volume);

}

}

}

}

domain和storagePool在被系统从数据库里彻底删除之前会一直保持为Deleting状态。TimeoutSweeper定时器会检查他们在Deleting状态持续到一定的时间之后，便会分别从domainStore和storagePoolStore里彻底删除。持续的时间段值nextActionTimeIntervalMs在系统部署时自己配置，一般为3秒。代码实现如下：

// process domain store

List\<Domain\> allDomains = domainStore.listAllDomains();

for (Domain domain : allDomains) {

if (domain.getStatus() == Status.Deleting) {

if (domain.timePassedLongEnough(nextActionTimeIntervalMs)) {

domainStore.deleteDomain(domain.getDomainId());

}

}

}

// process storage pool store

List\<StoragePool\> allStoragePools =
storagePoolStore.listAllStoragePools();

for (StoragePool storagePool : allStoragePools) {

if (storagePool.getStatus() == Status.Deleting) {

if (storagePool.timePassedLongEnough(nextActionTimeIntervalMs)) {

storagePoolStore.deleteStoragePool(storagePool.getPoolId());

}

}

}

### 4.3.2VolumeSweeper定时器

VolumeSweeper定时器也属于infoCenter软件模块，定时处理卷的各种信息，更新卷的各种状态和动作，解除卷之间的clone（克隆即复制）关系，将扩展卷和根卷连接起来等。主要操作步骤如下：

（1）获取包括rootVolume和所有的childVolume的关系链，以及将要链接上去的prevVolume；

（2）对于prevVolume，将该volume的childVolumeid修改成即将链入的volume的id；

（3）对于链入的childVolume，根据prevVolume中第一个segment在逻辑卷中的位置即PositionOfFirstSegmentInLogicVolume，修改自身在逻辑卷中的位置，即PositionOfFirstSegmentInLogicVolume；

（4）对于rootVolume，修改其正在extending的值；

（5）将prev和链入的childVolume的信息更新到datanode中。

在3.1.2小节中了解到，克隆是卷的一个非常重要的需求。利用克隆功能可以为用户数据快速创建一个完全独立的副本，供多种用途使用，如数据抽取、查询、商业智能、数据备份归档等。infoCenter中有CloneRelationshipsStore来专门存储卷之间的克隆关系。当克隆结束时，无论克隆成功还是失败，克隆关系都要从数据库里删除。卷之间的克隆关系的解除实现代码如下：

//delete cloneRelationship from cloneRelationshipsStore after clone

//whatever the clone operation is successful or failure

if (volume.isRoot()) {

if (volume.getVolumeSource().equals

(VolumeMetadata.VolumeSourceType.CLONE_VOLUME)) {

long rootId = volume.getRootVolumeId();

> List\<VolumeMetadata\> volumesFromRoot =
> volumeStore.listVolumesFromRoot(rootId);

boolean cloneRelationshipNeedDelete = true;

for (VolumeMetadata volumeToCheckStatus : volumesFromRoot) {

VolumeStatus volumeCheckStatus = volumeToCheckStatus.getVolumeStatus();

//when the cloneVolumeStatus is ToBeCreated or Creating,

//we don\'t delete cloneRelationship from cloneRelationshipsStore

if (VolumeStatus.ToBeCreated == volumeCheckStatus

\|\| VolumeStatus.Creating == volumeCheckStatus) {

cloneRelationshipNeedDelete = false;

break;

}

}

if (cloneRelationshipNeedDelete) {

long cloneVolumeId = volume.getVolumeId();

if (cloneRelationshipsStore.getFromDB(cloneVolumeId) != null) {

logger.warn(\"{} volume is done with clone, delete its clone
relationship\", cloneVolumeId);

cloneRelationshipsStore.deleteFromDB(cloneVolumeId);

}

}

}

} else {

linkToRoot(volume);

}

卷的扩展主要是将原来卷即根卷的存储容量扩大，即在同一存储池中又开辟出一段空间创建一个子卷，原来的卷作为父卷与子卷合并成一个卷呈现给用户，除了存储容量变大，其余的属性值与原来父卷的属性值相同。根卷即父卷，扩展卷即子卷。4.1.2中已经给出父卷和子卷的关系。同一存储池中rootVolumeId相同的卷在呈现给用户时的形态是经过合并后的一个整卷，而在系统内部作一些相关卷的操作时仍然是父卷和子卷分开的形态。因此，当父卷处于删除操作即状态为Deleting时，rootVolumeId与它形同的子卷也需要进行删除操作；或者子卷处于Deleting状态时，父卷也需要将状态转换为Deleting。在将子卷和父卷做连接操作时，最关键的是在数据库中找到rootVolumeId相同的卷，及时更新各个卷在合并之后的卷内的逻辑位置position。同时也要更新扩展的大小。核心代码实现如下：

VolumeMetadata root = volumes.get(0);

if (root.isDeletedByUser()) {

// set the volume deleting, next time data node report segment unit

// to info center, info center will notify data node to delete

// segment unit

volumeMetadata.setVolumeStatus(VolumeStatus.Deleting);

}

//更新各个卷在合并后的卷内的逻辑位置position

synchronized (volumeMetadata) {

int positionOfFirstSegInLogicVolume =
volumeMetadata.getPositionOfFirstSegmentInLogicVolume();

int newPositionOfFirstSegInLogicVolume =

prev.getPositionOfFirstSegmentInLogicVolume() + prev.getSegmentCount();

volumeMetadata.setPositionOfFirstSegmentInLogicVolume

(newPositionOfFirstSegInLogicVolume);

volumeMetadata.incVersion();

try {

volumeStore.saveVolume(volumeMetadata);

} catch (Exception e) {

logger.error(

\"Caught an exception when updating volume metadata to the database.
Reset everything: {}\", volumeMetadata, e);

volumeMetadata.setPositionOfFirstSegmentInLogicVolume

(positionOfFirstSegInLogicVolume);

volumeMetadata.decVersion();

return;

}

}

//更新扩展的大小

synchronized (root) {

long extendingSizeInRoot = root.getExtendingSize();

if (extendingSizeInRoot \< volumeMetadata.getVolumeSize()) {

root.setExtendingSize(0);

} else {

root.setExtendingSize(extendingSizeInRoot -
volumeMetadata.getVolumeSize());

}

// save the changes of extending size

try {

volumeStore.updateExtendingSize(root.getVolumeId(),
root.getExtendingSize());

} catch (Exception e) {

logger.error(\"Caught an exception when updating volume metadata to the
database. Reset everything: rootVolume {}\",root, e);

root.setExtendingSize(extendingSizeInRoot);

return;

}

}

### 4.3.3元数据重构接口

元数据重构接口reportSegmentUnitsMetadata属于InfoCenter软件模块，它主要负责把存储系统中所有datanode汇报上来的segmentUnit整理重构成卷volume的逻辑形态，存储在volumeStore中并提供给用户使用。对于接收到的segmentUnit，若该segmentUnit所属的volume存在，则转入processSegmentUnitWithVolumeExist接口；若该segmentUnit所属的volume不存在，则转入processSegmentUnitWithoutVolumeExist，并且根据该segmentUnit创建出一个volume存入到volumeStore中，代码实现如下：

if (volumeMetadata != null) {

try {

volumeMetadata = processSegmentUnitWithVolumeExist(volumeMetadata,
segUnit);

} catch (Exception e) {

logger.warn(\"catch an exception\", e);

processExceptionWithSegmentUnitReport(conflictSegmentUnits,
segUnit.getSegId(), e);

}

} else {

volumeMetadata = processSegmentUnitWithoutVolumeExist(segUnit);

}

if (volumeMetadata == null) {

logger.debug(\"Still can not find volume after process the segment unit
{}\", segUnit);

returnedSegUnits.add(receivedUnitMetadata_Thrift);

} else {

if(receivedUnitMetadata_Thrift.getStatus().equals

(SegmentUnitStatus_Thrift.Primary)) {

updateSegmentsFreeRatio(receivedUnitMetadata_Thrift, volumeMetadata);

}

volumeStore.saveVolume(volumeMetadata);

}

processSegmentUnitWithVolumeExist接口中，segmentUnit所属的volume已经存在，则判断volume是否为Dead状态。当volume不为Dead状态时，则将segmentUnit的相关信息更新到元数据库中；当volume处于Dead状态时，若segmentUnit所属的segment处于Deleting或Deleting状态时，则什么也不做，若该segment不处于Deleted或Deleting状态，则将该segmentUnit放在删除队列并通知datanode将其删除掉。processSegmentUnitWithoutVolumeExist接口中，汇报上来的segmentUnit所属的volume不存在，则检查该segmentUnit的segId（即segment在volume中的逻辑位置）是否为0，若为0，则该segmentUnit里面包含它所属的volume的所有元数据信息，按照这些信息创建一个新卷存储到volumeStore中；若不为0，则什么也不做。具体流程如图4-12所示：

图4-12 元数据重构流程图

## 4.4本章小结

在上一章对元数据管理方案进行了总体设计的基础上，本章根据设计中的需求和元数据管理基本流程的内容，首先给出了元数据组织形式和库中表结构的设计，以及一些重要属性的含义；其次介绍了元数据各个状态的具体含义，详细阐述了卷状态迁移和驱动过程；最后，介绍了元数据管理程序中两个定时器和一个元数据重构接口的实现过程。

# 第5章 元数据管理系统测试

## 5.1测试环境与方法

元数据管理属于ZettastorDBS存储系统中的一个功能模块，对它进行测试则需要部署整个ZettastorDBS系统。由于ZettastorDBS系统中的控制台console模块是做成web形式，故对元数据的测试过程和结果可以在网页中直接看到。整个测试环境是在局域网中搭建，使用了5台服务器，服务器配置统一，CPU是Intel(R)XeonR
4core，内存16G，500GB硬盘，千兆网卡。每台服务器上都安装了Ubuntu
12.04操作系统，使用版本号为9.3.10的postgresql作为本地数据库。

## 5.2系统测试结果

1.环境部署

测试环境中共5台服务器，IP地址范围为10.0.2.90至10.0.2.94。将Console、ControlCenter和InfoCenter部署在10.0.2.94上，4个存储节点DataNode部署在10.0.2.90至10.0.2.93上。将系统中所有软件模块放在同一个分支下，打包，解压并更新配置目录，在pengyun_deploy目录下输入以下命令：

bin/deploy.pl --o deploy

bin/zookeeper.pl

bin/daemonClient.pl --c deploy:all

部署成功后控制台显示如图5-1所示，各模块部署均为"SUCCESS"状态，总用时为16分43秒。

![](media/image20.jpg){width="5.620833333333334in"
height="4.645833333333333in"}图5-1 系统部署成功控制台情况

![](media/image21.jpg){width="4.104166666666667in"
height="3.6638888888888888in"}以管理员的账号admin和密码admin登录console，也可以看到部署成功后的整个系统各软件模块在所有服务器上的部署状况，如图5-2所示：

图5-2 各服务器部署情况

2.创建和查询元数据

创建一个名为"domain"的域，包含所有的datanode节点，即IP地址为10.0.2.90至10.0.2.93的服务器。domain创建成功后的页面如图5-3所示。将来自10.0.2.90至10.0.2.92的三块不同的磁盘组成一个名为"test"的存储池，将来自10.0.2.91至10.0.2.93的三块不同的磁盘组成一个名为"dd"的存储池。这样，domain中包含了两个存储池，即test和dd。存储池test创建成功后的页面如图5-4所示。

![](media/image22.jpg){width="5.768055555555556in"
height="2.2194444444444446in"}![](media/image23.jpg){width="5.768055555555556in"
height="2.4277777777777776in"}图5-3 创建域domain

图5-4 创建存储池test

在存储池test中创建3个卷，名称分别为v1、vv和test，大小均设置为1个segment（配置里默认1个segment的大小为1024MB），即1024MB，所属域为domain。在存储池dd中创建一个名为555的卷，大小也设置为1个segment，即1024MB，所属域为domain。同时，从卷vv克隆出卷vv-clone，所属存储池还是为test。此时domain中所有卷的信息列表页面如图5-5所示，列表中前4个卷的创建类型为创建，vv-clone的创建类型为克隆。列表中前3个卷状态为可用，卷vv的状态为被克隆中，卷v-clone的状态为克隆中。

![](media/image24.jpg){width="5.768055555555556in"
height="1.9826388888888888in"}图5-5 domain中的卷列表

3.删除元数据

![](media/image25.jpg){width="5.4625in"
height="2.066666666666667in"}若某个存储池不为空，还包含着卷，则该存储池不可被删掉。同理，不为空的域也不可被删掉。存储池和域的删除操作同卷的删除操作类似，下面讲述卷的删除操作。选中要删除的卷，点击删除按钮并确认，一段时间过后选中的卷的状态为已删除，如图5-6、5-7所示，选中删除卷vv和vv-clone，此时两个卷的状态为删除中。同一时间可以删除多个卷。

![](media/image26.jpg){width="5.499305555555556in"
height="2.134027777777778in"}图5-6 删除多个卷

图5-7 卷删除成功

4.更新元数据

下面讲述扩展卷的操作。选中卷v1，点击扩展按钮，输入扩展大小为,2个segment，如图5-8所示。扩展过程中v1的卷状态为扩展中。扩展成功后如图5-9![](media/image27.jpg){width="5.768055555555556in"
height="3.2625in"}所示，可以清楚看到，卷v1的大小从1024MB变为3072MB。

![](media/image28.jpg){width="5.768055555555556in"
height="2.316666666666667in"}图5-8 扩展卷V1

图5-9 卷V1扩展成功

## 5.3本章小结

本章描述了测试的硬件环境、软件环境以及测试方法，在局域网中部署了整个ZettastorDBS系统，针对系统中的元数据部分进行功能性测试，即对系统中的域、池和卷三种元数据进行增删查改的操作，结果表明本文提出的元数据管理技术具有准确性和可用性。

# 第6章 总结与展望

## 6.1工作总结

现有的分布式存储系统中的元数据管理模型都存在较严重的性能缺陷，无法满足人们对分布式存储系统越来越高的要求。分布式存储系统需求中比较重要的是高可用性、高可靠性和高可扩展性。本文根据已有的元数据管理模型和南京鹏云网络科技有限公司的项目需求，研究并实现了一种分布式存储系统中的元数据管理技术，主要研究工作如下：

（1）介绍了课题研究背景和研究意义，阐述了课题的来源，分析了现有的分布式存储系统中的元数据管理模型，将它们分成集中式元数据管理模型、分布式元数据管理模型和无元数据管理模型三种模型，分析各自的模型架构和缺陷，同时也介绍了分布式存储系统中的两种容错机制以及各自的优缺点。

（2）根据南京鹏云网络科技有限公司的具体需求，研究分布式存储系统ZettastorDBS的特点和组成，以及在生活中的应用，将ZettastorDBS系统总体架构分成两个模块：数据通路模块和控制通路模块。数据通路模块包括客户端驱动、Coordinator、DriverContainer和DataNode，控制通路模块包括信息中心InfoCenter、控制中心ControlCenter和控制台Console。同时根据分布式存储系统元数据管理需求给出元数据管理基本流程所包含的内容。

（3）实现了ZettastorDBS系统中的元数据管理技术。给出元数据的组织形式以及元数据库中表结构的设计，详细描述数据块segment、卷volume、存储池storagePool和域domain的各个状态表示的含义以及volume各状态之间的迁移转换图，并对迁移过程和元数据管理技术中的核心程序给出实现过程。同时，对系统进行功能测试，表明本文实现的元数据管理技术具有准确性和可用性。

## 6.2工作展望

本文实现了ZettastorDBS系统中的元数据管理功能，满足了具体的项目需求。但在测试过程中，还发现许多需要完善的地方，主要有以下两点：

（1）元数据管理部分对整个系统性能的影响。本文只是针对元数据管理部分的需求作基础操作流程的验证，没有和其他分布式存储系统作比较，无法衡量本文提出的元数据管理技术在性能上有提高多少，性能验证也是下一步有待完善的方向。

（2）高并发下的性能测试。本文测试验证中，客户量和实际情况没法比较的。在实际的生产环境中，成千上万的并发量是常态，因此如何针对高并发对系统进行优化，提高系统在高并发下的性能，也是下一阶段的完善方向。

# 参考文献

1.  徐宗本.
    数据分析与处理的共性基础与核心技术，第四届中国计算机学会(CCF)大数据学术会议，兰州.2016.10.11-13.

2.  魏青松. 大规模分布式存储技术研究\[D\].电子科技大学,2004.

3.  Koyluoglu O O, Rawat A S,Vishwanath S. Secure Cooperative
    Regenerating Codesfor Distributed Storage Systems\[J\].IEEE
    Transactions on Information Theory, 2012,60(9):5228-5244.

4.  王燕楠.
    基于Hadoop的海量医学影像数据处理过程中的优化方法研究\[D\].首都师范大学,2014.

5.  Liu X, Han J, Zhong Y, et al. Implementing WebGIS on Hadoop: A case
    study of improving small file I/O performance on HDFS\[C\]// IEEE
    International Conference on CLUSTER Computing and Workshops. IEEE,
    2009:1-8.

6.  Bo Dong, Jie Qiu, Qinghua Zheng, Xiao Zhong, Jingwei Li, Ying Li. A
    Novel Approach to Improving the Efficiency of Storing and Accessing
    Small Files on Hadoop:A Case Study by PowerPoint Files\[C\].In
    Proceedings of IEEE SCC, 2010: 65-72.

7.  黄朝光.
    面向农业科学数据的分布式存储系统的研究与实现\[D\].北京工业大学,2015.

8.  梁堃. 分布式教育云存储平台的设计与实现\[D\].华南理工大学,2016.

9.  杨红霞,赵改善.
    21世纪的地震数据处理系统\[J\].石油物探,2001(04):125-140.

10. Ranganath B N,Murty M N. Stream-C1ose:Fast Mining of Closed
    FrequentTtemsets in Hige Speed Data Stream\[C\] //Proc of the 2008
    IEEE International Conference on Data Mining
    Workshops,2008.Pisa,Italy:IEEE Computer Society,2008:516-525.

11. Star S. Categories and Cognition: Material and Conceptual Aspects of
    Large Scale Category Systems \[J\].Interdisciplinary collaboration:
    An emerging cognitive science, 2005: 167-186.

12. Deelman E, Singh G, Su M-H, et al. Pegasus: A framework for mapping
    complex scientific workflows onto distributed systems
    \[J\].Scientific Programming, 2005, 13(3): 219-237.

13. 徐熙超.
    基于HBase的海量气象结构化数据查询优化\[D\].南京信息工程大学,2016.

14. 张宇露. 分布式视频流存储系统的设计与实现\[D\].电子科技大学,2016.

15. 田晓艳. 云环境下高考视频监控数据管理技术研究\[D\].山东师范大学,2013.

16. Chang F, Dean J, Ghemawat S, et al. Bigtable: a distributed storage
    system for structured data\[J\]. ACM Transactions on Computer
    Systems, 2008, 26(2):1-26.

17. Weil S A, Brandt S A, Miller E L, etal. Ceph: A scalable,
    high-performance distributed file system\[C\]//Proceedings of the
    7th symposium on Operating systems design and implementation. USENIX
    Association,2006: 307-320.

18. Borthakur D,Gray J,Sarma J S,et al.Apache Hadoop goes realtime
    atFacebook\[C\]//Proceedings of the 2011 ACM SIGMOD International
    Conference on Management of data. ACM, 2011:1071一1080.

19. Rabe B R, Clifford M, Miles N. Storage area network (SAN) management
    system for discovering SAN components using a SAN management server:
    US, US 7194538 B1\[P\]. 2007.

20. Gibson G A, Uan Meter R. Network attached storage architecture\[J\].
    Communications of the ACM, 2000, 43(11):37-45.

21. 许金乐. 面向大数据的分布式存储关键技术的研究\[D\].东南大学,2016.

22. 百度百科GFS http://baike.baidu.com/view/805525.htm\#1

23. 陈虎. 基于HDFS的云存储平台的优化与实现\[D\].华南理工大学,2012.

24. Parallel Virtual File System,Version 2,http://www.pvfs.org/

25. Wang F, Chen Y, Li S, et al. The design of data storage system based
    on Lustre for EAST\[J\]. Fusion Engineering & Design, 2016,
    112:961-963.

26. Vaidya M. Critical Study of Performance Parameters on Distributed
    File Systems Using MapReduce\[C\]// International Conference on
    Information Security and Privacy. 2015.

27. Yianilos P N, Sobti S. The evolving field of distributed
    storage\[J\]. IEEE Internet Computing, 2001, 5(5):35-39.

28. R.Rivest.The MDS Message-Digest Algorithm\[EB/OL\].
    https://tools.ietf.org/html/rfc1321,March 7,2014

29. Dink.IDC :
    2011年全球数据产生量达到1.BZB到2020年将增长50倍.http://www.199it.com/archives/12227.htm1

30. 王敏. 制造业大数据分布式存储管理方法研究\[D\].武汉大学,2017.

31. M Szeredi. FUSE: Filesystem in userspace\[J\]. Accessed on, 2010

32. 张兴. 基于Hadoop的云存储平台的研究与实现\[D\];电子科技大学，2013.

33. Hadoop\[EB/OL\]. http://hadoop.apache.org/core/, 2008.12.16.

34. 黄妍.
    基于Hadoop的气象信息云存储系统设计与实现\[D\].电子科技大学,2016.

35. 申爱花.
    基于分布式文件系统GlusterFS的安全技术研究\[D\].华中科技大学,2012.

36. 易理林.
    HDFS文件系统中元数据的高可用性管理方法研究\[D\].华南理工大学,2013.

37. Yang J. Research on Distributed Storage Technology Based on Mass
    Data\[J\]. Applied Mechanics & Materials, 2014, 687-691:2710-2713.

38. D Karger, A Sherman, A Berkheimer, et al. Web caching with
    consistent hashing\[J\]. Computer Networks, 1999, 31(11): 1203一1213

39. 冯幼乐.
    分布式文件系统元数据管理技术研究与实现\[D\].中国科学技术大学,2010

40. I Stoica, R Morris, D Karger, et al. Chord: A scalable peer-to-peer
    lookup service for Internet applications\[J\].ACM SIGCOMM Computer
    Communication Review, 2001, 31(4): 149-160

41. 张晓林. 元数据研究与应用\[M\]. 书目文献出版社, 2002.

42. 李芬，朱志祥，刘盛辉. 大数据发展现状及面临的问题\[J\].
    西安邮电大学学报, 2013, 12(5):100-103.

43. M Mesnier, G R Ganger, E Riedel. Object-based storage\[J\].
    Communications Magazine, IEEE,2003, 41(8): 84-90.

44. Shreedhar M, Uarghese G Efficient fair queuing using deficit
    round-robin\[J\].Networking, IEEE/ACM Transactions on,1996,4(3):
    375-385.

45. Douglas C, Stephen D. C++ Network Programming:Systematic Reuse with
    ACE and Frmeworks\[M\]. Addison Wesley,2003:39-80

46. select:http://en.wikipedia.org/wild/Select (Unix)

47. Olson M A, Bostic K, Seltzer M I. Berkeley DB\[C\]//USENIX Annual
    Technical Conference, FREENIX Track. 1999: 183-191.

48. Wikipedia. Epoll\[EB/OL\]. http://en.wikipedia.org/wiki/Epoll, March
    7, 2014

49. Google. Protocol Buffers-Google\'s data interchange format\[EB/OL\].
    https://code.google.com/p/protobuf/, March 7, 2014

50. Li J,Wang X, Li B.Pipelined Regeneration with Regenerating Codes for
    Distributed Storage Systems\[C\].Network Coding(NetCod),2011
    International Symposium on. IEEE,2011:1-6.

# 致谢

光阴荏苒，岁月如梭。一眨眼，两年的硕士研究生生涯正渐渐走向尾声。在这论文即将完成之日，我感慨颇多。回顾过去的时光，有成功的喜悦，亦有失败的沮丧，有太多的人和事值得我去记忆。

首先，把最衷心的感谢和最诚挚的敬意送给我的导师吉根林教授！在我攻读硕士学位期间，他从题目的选择、开题报告的撰写、研究的进展、实验的设计，特别是学术论文的撰写等诸多方面都给予了我悉心、睿智地指导，为我的课题研究和论文写作付出了大量的心血，督促我不断地进步。吉老师以他严谨的治学态度、始终如一的工作热情、扎实的理论基础、广博的知识结构、敏锐的学术洞察能力和精益求精的学术追求，使我不仅在学习和研究中受益颇深，更在为人处世上给予我莫大的启发。学习上循循善诱的言传身教，指导勉励我不断进步；生活上无微不至的关怀，帮助支持我永远向前。我会一直谨记吉老师的谆谆教诲，在未来工作生活中取得更大的进步。

同时，要感谢课题组的鲍培明老师、赵斌老师、谈超老师在学习和生活上对我的关心、指导和帮助。在日常与他们的交流过程中，我从他们身上学到了很多，沟通能力和技术能力都得到了很大的提升。

其次，感谢南京鹏云网络科技有限公司的陈总给我提供了一个良好的实习平台，感谢实习期间的实习导师王中原老师，同事刘亚辉、高姣姣、王静。感谢你们不厌其烦地帮我解决在实习期间遇到的问题，时常给我传授工作上的经验，使我快速适应企业环境，了解企业文化，融入到鹏云这个集体当中去。我会非常怀念和你们在一起工作、学习的日子。

再次，感谢同师门的师兄师姐和师弟师妹，张逸凡，吕晟斓，赵竹珺，孙弘艳，曹忠义，杨宇，张玉洁，史覃覃，徐成，曹艳秋，陶宁等。在学习上跟他们一起讨论，给了我很多启发，在生活上接受了他们很多帮助，我深受感动。因为有了他们的帮助和陪伴，紧张的学习生活充满了欢声笑语。

特别的，我要感谢我的父母，他们为了我的成长默默地倾注了无数的心血，他们始终如一的关爱和理解，是我不断前进的精神动力。正是有了他们默默的支持，我才能够在面对生活和学习中所遇到的种种困难时，始终坚持着追求自己的梦想。

最后，对所有参加论文评审和对论文提出宝贵意见的各位专家教授以及老师们表示最真挚的感谢，衷心地祝你们身体健康，万事如意！
